# .env.example
# This file provides examples for configuring Clippy 2.Oh's AI backend.
# Copy this file to a new file named `.env` and uncomment/fill in
# the variables according to your chosen AI provider.

OPENAI_API_KEY="YOUR_API_KEY_HERE"
OPENAI_MODEL="YOUR_MODEL_NAME_HERE"


# REQUIRED VARIABLES
# OPENAI_API_KEY: Your API key. Required for most providers (OpenAI, OpenRouter).
#                 For some local servers (like LM Studio), you might put any string or "lm-studio".

# OPENAI_MODEL: The specific model name you want to use. This is REQUIRED.
#               Check your API provider's documentation for available models.
# Examples: "gpt-3.5-turbo", "gpt-4", "mistralai/mistral-7b-instruct", "local-model"

# OPTIONAL VARIABLES
# OPENAI_API_BASE: The base URL for the OpenAI-compatible API endpoint.
#                  Defaults to "https://api.openai.com/v1" if not set.
#                  Only set this if you are NOT using the official OpenAI API.
#
# Example for OpenRouter.ai:
# OPENAI_API_BASE="https://openrouter.ai/api/v1"
#
# Example for a local LM Studio server (check the port in LM Studio):
# OPENAI_API_BASE="http://localhost:1234/v1"
#
# Example for a local Ollama server with an OpenAI-compatible proxy (like LiteLLM):
# OPENAI_API_BASE="http://localhost:4000" # Or the port your proxy uses

# Example for official OpenAI API (no need to uncomment if using the default base):
# OPENAI_API_BASE="https://api.openai.com/v1"