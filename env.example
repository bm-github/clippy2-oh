# .env.example
# This is an example environment file.
# Copy this file to .env and fill in your actual credentials and settings.
# Do NOT commit your actual .env file with real secrets to version control.

# --- OpenAI API Configuration ---
# This can be an API key from OpenAI, OpenRouter, or other compatible services.
# For some local servers (like LM Studio default), this might be ignored or can be a placeholder.
OPENAI_API_KEY="YOUR_API_KEY_HERE"

# The base URL of the OpenAI-compatible API endpoint.
# Examples:
# For OpenRouter:       OPENAI_API_BASE="https://openrouter.ai/api/v1"
# For LM Studio (local): OPENAI_API_BASE="http://localhost:1234/v1"
# For Ollama (local, via LiteLLM or similar proxy): OPENAI_API_BASE="http://localhost:8000/v1" (or whatever your proxy uses)
# For OpenAI direct:    OPENAI_API_BASE="https://api.openai.com/v1"
OPENAI_API_BASE="https://openrouter.ai/api/v1"

# The model identifier to use.
# Examples:
# For OpenRouter:       OPENAI_MODEL="mistralai/mistral-7b-instruct" or "openai/gpt-3.5-turbo" or "google/gemini-pro"
# For LM Studio (local): OPENAI_MODEL="local-model" (often a placeholder, server uses the loaded model)
# For Ollama (local):   OPENAI_MODEL="llama2" or "mistral" (the name of the model you pulled with ollama)
# For OpenAI direct:    OPENAI_MODEL="gpt-3.5-turbo" or "gpt-4"
OPENAI_MODEL="mistralai/mistral-7b-instruct"